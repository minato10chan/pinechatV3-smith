from typing import List, Dict, Any, Tuple
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, AIMessage, SystemMessage
import os
import tiktoken
from openai import OpenAI
from ..config.settings import (
    PINECONE_API_KEY,
    PINECONE_INDEX_NAME,
    OPENAI_API_KEY,
    DEFAULT_TOP_K,
    SIMILARITY_THRESHOLD,
    DEFAULT_SYSTEM_PROMPT,
    DEFAULT_RESPONSE_TEMPLATE,
    ENABLE_HYBRID_SEARCH,
    ENABLE_QUERY_EXPANSION
)
from .pinecone_service import PineconeService
from ..utils.text_processing import expand_educational_query

class LangChainService:
    def __init__(self, callback_manager=None):
        """LangChainã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.openai_client = OpenAI(api_key=OPENAI_API_KEY)
        
        # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.llm = ChatOpenAI(
            api_key=OPENAI_API_KEY,
            model_name="gpt-4o-mini",
            temperature=0.85,
            callback_manager=callback_manager
        )
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embeddings = OpenAIEmbeddings(
            api_key=OPENAI_API_KEY,
            model="text-embedding-3-large",
            dimensions=3072
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        
        # Pineconeã®APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
        os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY
        
        # Pineconeãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
        self.vectorstore = PineconeVectorStore.from_existing_index(
            index_name=PINECONE_INDEX_NAME,
            embedding=self.embeddings
        )
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
        self.message_history = ChatMessageHistory()
        
        # PineconeServiceã®åˆæœŸåŒ–ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ï¼‰
        self.pinecone_service = PineconeService()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.system_prompt = DEFAULT_SYSTEM_PROMPT
        self.response_template = DEFAULT_RESPONSE_TEMPLATE

    def check_api_usage(self):
        """OpenAI APIã®ä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª"""
        try:
            # ä½¿ç”¨çŠ¶æ³ã®å–å¾—
            # usage = self.openai_client.usage.retrieve()
            
            # ä½¿ç”¨çŠ¶æ³ã®è¡¨ç¤º
            print("\n=== OpenAI API Usage ===")
            # print(f"Total Tokens: {usage.total_tokens}")
            # print(f"Total Cost: ${usage.total_cost:.4f}")
            # print(f"Usage Period: {usage.period}")
            
            # ã‚¯ã‚©ãƒ¼ã‚¿æƒ…å ±ã®å–å¾—
            # quota = self.openai_client.quota.retrieve()
            print("\n=== OpenAI API Quota ===")
            # print(f"Total Quota: ${quota.total_quota:.2f}")
            # print(f"Used Quota: ${quota.used_quota:.2f}")
            # print(f"Remaining Quota: ${quota.remaining_quota:.2f}")
            # print(f"Quota Period: {quota.period}")
            
            # è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            # if quota.remaining_quota < 1.0:
            #     print("\nâš ï¸ Warning: Remaining quota is less than $1.0")
            # if quota.remaining_quota < 0.1:
            #     print("ğŸš¨ Critical: Remaining quota is less than $0.1")
                
        except Exception as e:
            error_message = str(e)
            print(f"\nâŒ Error checking API usage: {error_message}")
            
            if "insufficient_quota" in error_message:
                print("\nğŸš¨ Critical: API quota has been exceeded!")
                print("Please check your OpenAI API key and billing settings.")
                print("You can check your usage and quota at: https://platform.openai.com/account/usage")
            elif "object has no attribute" in error_message:
                print("\nâš ï¸ Warning: Unable to check API usage. This might be due to API changes or permissions.")
                print("Please check your OpenAI API key and ensure it has the necessary permissions.")
            else:
                print("\nâš ï¸ Warning: Unable to check API usage. Please verify your API key and permissions.")

    def count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return len(self.encoding.encode(text))

    def get_relevant_context(self, query: str, top_k: int = DEFAULT_TOP_K, similarity_threshold: float = SIMILARITY_THRESHOLD, enable_hybrid: bool = None, enable_expansion: bool = None) -> Tuple[str, List[Dict[str, Any]], int]:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡è„ˆã‚’å–å¾—ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ãƒ»ã‚¯ã‚¨ãƒªæ‹¡å¼µå¯¾å¿œï¼‰"""
        try:
            if enable_hybrid is None:
                enable_hybrid = ENABLE_HYBRID_SEARCH
            if enable_expansion is None:
                enable_expansion = ENABLE_QUERY_EXPANSION
            
            query_tokens = self.count_tokens(query)
            print(f"ã‚¯ã‚¨ãƒªã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {query_tokens}")
            
            original_query = query
            if enable_expansion:
                expanded_query = expand_educational_query(query)
                print(f"å…ƒã®ã‚¯ã‚¨ãƒª: {original_query}")
                print(f"æ‹¡å¼µã‚¯ã‚¨ãƒª: {expanded_query}")
                query = expanded_query
            
            if enable_hybrid:
                search_results = self.pinecone_service.query(
                    query_text=query,
                    top_k=top_k,
                    similarity_threshold=similarity_threshold,
                    enable_hybrid=True
                )
                
                simplified_docs = []
                for match in search_results.get('matches', []):
                    simplified_metadata = {}
                    metadata = match.metadata or {}
                    
                    for key, value in metadata.items():
                        if isinstance(value, str):
                            simplified_metadata[key] = value[:100] + "..." if len(value) > 100 else value
                    
                    content = metadata.get('text', '')
                    if len(content) > 500:
                        content = content[:500] + "..."
                    
                    simplified_doc = {
                        "content": content,
                        "metadata": simplified_metadata,
                        "score": match.score
                    }
                    simplified_docs.append(simplified_doc)
            else:
                docs = self.vectorstore.similarity_search_with_score(query, k=top_k)
                
                simplified_docs = []
                for doc in docs:
                    simplified_metadata = {}
                    for key, value in doc[0].metadata.items():
                        if isinstance(value, str):
                            simplified_metadata[key] = value[:100] + "..." if len(value) > 100 else value
                    
                    content = doc[0].page_content
                    if len(content) > 500:
                        content = content[:500] + "..."
                    
                    simplified_doc = {
                        "content": content,
                        "metadata": simplified_metadata,
                        "score": doc[1]
                    }
                    simplified_docs.append(simplified_doc)
            
            filtered_docs = [
                doc for doc in simplified_docs
                if doc["score"] >= similarity_threshold
            ]
            
            print(f"ã—ãã„å€¤ä»¥ä¸Šã®å€™è£œæ•°: {len(filtered_docs)}")
            if filtered_docs:
                print("æ¡ç”¨ã•ã‚ŒãŸå€™è£œã®ã‚¹ã‚³ã‚¢:")
                for doc in filtered_docs:
                    print(f"ã‚¹ã‚³ã‚¢: {doc['score']:.3f}, ãƒ†ã‚­ã‚¹ãƒˆ: {doc['content'][:100]}...")
            else:
                print("ã—ãã„å€¤ä»¥ä¸Šã®å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            context_text = "\n".join([doc["content"] for doc in filtered_docs])
            
            context_tokens = self.count_tokens(context_text)
            print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {context_tokens}")
            
            search_details = []
            for doc in filtered_docs:
                detail = {
                    "ã‚¹ã‚³ã‚¢": round(doc["score"], 4),
                    "ãƒ†ã‚­ã‚¹ãƒˆ": doc["content"][:100] + "...",
                    "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿": doc["metadata"],
                    "ãƒ•ã‚¡ã‚¤ãƒ«å": doc["metadata"].get("source", "ä¸æ˜"),
                    "ãƒšãƒ¼ã‚¸ç•ªå·": doc["metadata"].get("page", "ä¸æ˜"),
                    "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": doc["metadata"].get("section", "ä¸æ˜"),
                    "æ¤œç´¢æ–¹å¼": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰" if enable_hybrid else "æ„å‘³æ¤œç´¢ã®ã¿",
                    "ã‚¯ã‚¨ãƒªæ‹¡å¼µ": "æœ‰åŠ¹" if enable_expansion else "ç„¡åŠ¹"
                }
                search_details.append(detail)
            
            return context_text, search_details, context_tokens
            
        except Exception as e:
            error_message = str(e)
            if "insufficient_quota" in error_message:
                print("\nğŸš¨ Critical: API quota has been exceeded!")
                print("Please check your OpenAI API key and billing settings.")
                print("You can check your usage and quota at: https://platform.openai.com/account/usage")
                return "", [{
                    "ã‚¨ãƒ©ãƒ¼": True,
                    "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": "API quota has been exceeded",
                    "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "API Quota Error",
                    "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": "Please update your API key in Streamlit Cloud settings"
                }], 0
            else:
                print(f"\nâŒ Error in get_relevant_context: {error_message}")
                return "", [{
                    "ã‚¨ãƒ©ãƒ¼": True,
                    "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": error_message,
                    "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "Search Error"
                }], 0

    def get_response(self, query: str, system_prompt: str = None, response_template: str = None, property_info: str = None, chat_history: list = None, similarity_threshold: float = SIMILARITY_THRESHOLD, enable_hybrid: bool = None, enable_expansion: bool = None) -> Tuple[str, Dict[str, Any]]:
        """ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­å®š
            system_prompt = system_prompt or self.system_prompt
            response_template = response_template or self.response_template
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®ä½œæˆ
            messages = [
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("system", "å‚ç…§æ–‡è„ˆ:\n{context}")
            ]
            
            # ç‰©ä»¶æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            if property_info:
                messages.append(("system", "ç‰©ä»¶æƒ…å ±:\n{property_info}"))
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®è¿½åŠ 
            messages.append(("human", "{input}"))
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
            prompt = ChatPromptTemplate.from_messages(messages)
            
            # ãƒã‚§ãƒ¼ãƒ³ã®åˆæœŸåŒ–
            chain = prompt | self.llm
            
            # é–¢é€£ã™ã‚‹æ–‡è„ˆã‚’å–å¾—ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ãƒ»ã‚¯ã‚¨ãƒªæ‹¡å¼µå¯¾å¿œï¼‰
            context, search_details, context_tokens = self.get_relevant_context(
                query, 
                similarity_threshold=similarity_threshold,
                enable_hybrid=enable_hybrid,
                enable_expansion=enable_expansion
            )
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¨­å®š
            if chat_history:
                self.message_history.messages = []
                for role, content in chat_history:
                    if role == "human":
                        self.message_history.add_user_message(content)
                    elif role == "ai":
                        self.message_history.add_ai_message(content)
            
            # ä¼šè©±å±¥æ­´ã‚’æœ€é©åŒ–
            self.optimize_chat_history()
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            prompt_tokens = self.count_tokens(system_prompt)
            print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {prompt_tokens}")
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            history_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
            print(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {history_tokens}")
            
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼šé€ä¿¡ã•ã‚Œã‚‹ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
            print("\n=== é€ä¿¡ã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆ ===")
            print("\n--- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---")
            print(system_prompt)
            print("\n--- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ ---")
            for msg in self.message_history.messages:
                print(f"\n[{msg.type}]: {msg.content}")
            print("\n--- å‚ç…§æ–‡è„ˆ ---")
            print(context)
            if property_info:
                print("\n--- ç‰©ä»¶æƒ…å ± ---")
                print(property_info)
            print("\n--- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ---")
            print(query)
            
            # å¿œç­”ã‚’ç”Ÿæˆ
            response = chain.invoke({
                "chat_history": self.message_history.messages,
                "context": context,
                "property_info": property_info or "ç‰©ä»¶æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "input": query
            })
            
            # å¿œç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            response_tokens = self.count_tokens(response.content)
            print(f"å¿œç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {response_tokens}")
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            self.message_history.add_user_message(query)
            self.message_history.add_ai_message(response.content)
            
            # è©³ç´°æƒ…å ±ã®ä½œæˆ
            details = {
                "ãƒ¢ãƒ‡ãƒ«": "gpt-4o-mini",
                "ä¼šè©±å±¥æ­´": "æœ‰åŠ¹",
                "ãƒˆãƒ¼ã‚¯ãƒ³æ•°": {
                    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": prompt_tokens,
                    "ãƒãƒ£ãƒƒãƒˆå±¥æ­´": history_tokens,
                    "å‚ç…§æ–‡è„ˆ": context_tokens,
                    "ç‰©ä»¶æƒ…å ±": self.count_tokens(property_info) if property_info else 0,
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›": self.count_tokens(query),
                    "åˆè¨ˆ": prompt_tokens + history_tokens + context_tokens + (self.count_tokens(property_info) if property_info else 0)
                },
                "é€ä¿¡ãƒ†ã‚­ã‚¹ãƒˆ": {
                    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": system_prompt,
                    "ãƒãƒ£ãƒƒãƒˆå±¥æ­´": [{"type": msg.type, "content": msg.content} for msg in self.message_history.messages],
                    "å‚ç…§æ–‡è„ˆ": context,
                    "å‚ç…§æ–‡è„ˆã®è©³ç´°": search_details,
                    "ç‰©ä»¶æƒ…å ±": property_info,
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›": query
                }
            }
            
            return response.content, details
            
        except Exception as e:
            error_message = str(e)
            if "insufficient_quota" in error_message:
                error_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚APIã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚\n\n" + \
                               "ä»¥ä¸‹ã®æ‰‹é †ã§å¯¾å¿œã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼š\n" + \
                               "1. OpenAIã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„\n" + \
                               "2. æ–°ã—ã„APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãã ã•ã„\n" + \
                               "3. Streamlit Cloudã®è¨­å®šã§æ–°ã—ã„APIã‚­ãƒ¼ã‚’æ›´æ–°ã—ã¦ãã ã•ã„\n\n" + \
                               "è©³ç´°ã¯ã“ã¡ã‚‰ã§ç¢ºèªã§ãã¾ã™ï¼š\n" + \
                               "https://platform.openai.com/account/usage"
            else:
                error_response = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{error_message}"
            
            error_details = {
                "ã‚¨ãƒ©ãƒ¼": True,
                "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": error_message,
                "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "API Quota Error" if "insufficient_quota" in error_message else "Unknown Error"
            }
            
            return error_response, error_details

    def optimize_chat_history(self, max_tokens: int = 10000) -> None:
        """ä¼šè©±å±¥æ­´ã‚’æœ€é©åŒ–ã—ã€é‡è¦ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä¿æŒ"""
        if not self.message_history.messages:
            return

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ç¢ºä¿ï¼ˆç´„4000ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        reserved_tokens = 4000
        available_tokens = max_tokens - reserved_tokens

        # ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        current_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
        if current_tokens <= available_tokens:
            return

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é‡è¦åº¦ã§åˆ†é¡
        important_messages = []
        other_messages = []
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿æŒ
        for msg in self.message_history.messages:
            if isinstance(msg, SystemMessage):
                important_messages.append(msg)
                continue
            other_messages.append(msg)

        # æœ€æ–°ã®1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä¿æŒ
        if other_messages:
            important_messages.append(other_messages[-1])
            other_messages = other_messages[:-1]

        # é‡è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        important_tokens = sum(self.count_tokens(msg.content) for msg in important_messages)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        remaining_tokens = available_tokens - important_tokens

        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ã„ã¦ã€ä»–ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é•·ã•ã§ã‚½ãƒ¼ãƒˆï¼ˆçŸ­ã„ã‚‚ã®ã‹ã‚‰ï¼‰
        other_messages.sort(key=lambda x: self.count_tokens(x.content))
        
        for msg in other_messages:
            msg_tokens = self.count_tokens(msg.content)
            if msg_tokens <= remaining_tokens:
                important_messages.insert(0, msg)  # å…ˆé ­ã«è¿½åŠ 
                remaining_tokens -= msg_tokens
            else:
                break

        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§å±¥æ­´ã‚’æ›´æ–°
        self.message_history.messages = important_messages

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›
        final_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
        print(f"\n=== Chat History Optimization ===")
        print(f"Original tokens: {current_tokens}")
        print(f"Final tokens: {final_tokens}")
        print(f"Messages kept: {len(self.message_history.messages)}")
        print(f"Available tokens: {available_tokens}")
        print(f"Remaining tokens: {remaining_tokens}")

    def clear_memory(self):
        """ä¼šè©±ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"""
        self.message_history.clear()  