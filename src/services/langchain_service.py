from typing import List, Dict, Any, Tuple
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, AIMessage, SystemMessage
import os
import tiktoken
from openai import OpenAI
from ..config.settings import (
    PINECONE_API_KEY,
    PINECONE_INDEX_NAME,
    OPENAI_API_KEY,
    DEFAULT_TOP_K,
    SIMILARITY_THRESHOLD,
    DEFAULT_SYSTEM_PROMPT,
    DEFAULT_RESPONSE_TEMPLATE
)

class LangChainService:
    def __init__(self, callback_manager=None):
        """LangChainã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.openai_client = OpenAI(api_key=OPENAI_API_KEY)
        
        # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.llm = ChatOpenAI(
            api_key=OPENAI_API_KEY,
             # model_name="gpt-4o-mini",
            model_name="gpt-3.5-turbo",
            temperature=0.85,
            callback_manager=callback_manager
        )
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embeddings = OpenAIEmbeddings(
            api_key=OPENAI_API_KEY,
            model="text-embedding-ada-002"
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        
        # Pineconeã®APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
        os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY
        
        # Pineconeãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
        self.vectorstore = PineconeVectorStore.from_existing_index(
            index_name=PINECONE_INDEX_NAME,
            embedding=self.embeddings
        )
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
        self.message_history = ChatMessageHistory()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.system_prompt = DEFAULT_SYSTEM_PROMPT
        self.response_template = DEFAULT_RESPONSE_TEMPLATE

    def check_api_usage(self):
        """OpenAI APIã®ä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª"""
        try:
            # ä½¿ç”¨çŠ¶æ³ã®å–å¾—
            # usage = self.openai_client.usage.retrieve()
            
            # ä½¿ç”¨çŠ¶æ³ã®è¡¨ç¤º
            print("\n=== OpenAI API Usage ===")
            # print(f"Total Tokens: {usage.total_tokens}")
            # print(f"Total Cost: ${usage.total_cost:.4f}")
            # print(f"Usage Period: {usage.period}")
            
            # ã‚¯ã‚©ãƒ¼ã‚¿æƒ…å ±ã®å–å¾—
            # quota = self.openai_client.quota.retrieve()
            print("\n=== OpenAI API Quota ===")
            # print(f"Total Quota: ${quota.total_quota:.2f}")
            # print(f"Used Quota: ${quota.used_quota:.2f}")
            # print(f"Remaining Quota: ${quota.remaining_quota:.2f}")
            # print(f"Quota Period: {quota.period}")
            
            # è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            # if quota.remaining_quota < 1.0:
            #     print("\nâš ï¸ Warning: Remaining quota is less than $1.0")
            # if quota.remaining_quota < 0.1:
            #     print("ğŸš¨ Critical: Remaining quota is less than $0.1")
                
        except Exception as e:
            error_message = str(e)
            print(f"\nâŒ Error checking API usage: {error_message}")
            
            if "insufficient_quota" in error_message:
                print("\nğŸš¨ Critical: API quota has been exceeded!")
                print("Please check your OpenAI API key and billing settings.")
                print("You can check your usage and quota at: https://platform.openai.com/account/usage")
            elif "object has no attribute" in error_message:
                print("\nâš ï¸ Warning: Unable to check API usage. This might be due to API changes or permissions.")
                print("Please check your OpenAI API key and ensure it has the necessary permissions.")
            else:
                print("\nâš ï¸ Warning: Unable to check API usage. Please verify your API key and permissions.")

    def count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return len(self.encoding.encode(text))

    def get_relevant_context(self, query: str, top_k: int = DEFAULT_TOP_K) -> Tuple[str, List[Dict[str, Any]]]:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡è„ˆã‚’å–å¾—"""
        try:
            # ã‚¯ã‚¨ãƒªã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            query_tokens = self.count_tokens(query)
            print(f"ã‚¯ã‚¨ãƒªã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {query_tokens}")
            
            # ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_vector = self.embeddings.embed_query(query)
            
            # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—ã—ã¦ã€å¾Œã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            docs = self.vectorstore.similarity_search_with_score(query, k=top_k * 2)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚æ¤œç´¢å¯¾è±¡ã«å«ã‚ã‚‹
            for doc in docs:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œç´¢å¯¾è±¡ã«è¿½åŠ 
                metadata_text = []
                for key, value in doc[0].metadata.items():
                    if isinstance(value, str):
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å€¤ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
                        metadata_text.append(f"{key}: {value}")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã®å‰ã«è¿½åŠ 
                if metadata_text:
                    doc[0].page_content = "\n".join(metadata_text) + "\n\n" + doc[0].page_content
            
            # ã‚¹ã‚³ã‚¢ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_docs = []
            for doc in docs:
                if doc[1] >= SIMILARITY_THRESHOLD:
                    filtered_docs.append(doc)
                    if len(filtered_docs) >= top_k:
                        break
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®çµæœãŒ0ä»¶ã®å ´åˆã¯ã€ã‚¹ã‚³ã‚¢ã«é–¢ä¿‚ãªãä¸Šä½Kä»¶ã‚’ä½¿ç”¨
            if not filtered_docs and docs:
                filtered_docs = docs[:top_k]
            
            context_text = "\n".join([doc[0].page_content for doc in filtered_docs])
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            context_tokens = self.count_tokens(context_text)
            print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {context_tokens}")
            
            search_details = []
            for doc in filtered_docs:
                detail = {
                    "ã‚¹ã‚³ã‚¢": round(doc[1], 4),  # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’å°æ•°ç‚¹4æ¡ã¾ã§è¡¨ç¤º
                    "ãƒ†ã‚­ã‚¹ãƒˆ": doc[0].page_content[:100] + "...",  # ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤º
                    "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿": doc[0].metadata,  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    "é¡ä¼¼åº¦åˆ¤æ–­": {
                        "é–¾å€¤": SIMILARITY_THRESHOLD,
                        "é–¾å€¤ä»¥ä¸Š": doc[1] >= SIMILARITY_THRESHOLD,
                        "ã‚¹ã‚³ã‚¢è©³ç´°": f"ã‚¹ã‚³ã‚¢ {round(doc[1], 4)} ã¯é–¾å€¤ {SIMILARITY_THRESHOLD} ã«å¯¾ã—ã¦ {'ä»¥ä¸Š' if doc[1] >= SIMILARITY_THRESHOLD else 'æœªæº€'}",
                        "ç†è§£éç¨‹": {
                            "ã‚¯ã‚¨ãƒª": query,
                            "ãƒ†ã‚­ã‚¹ãƒˆ": doc[0].page_content,
                            "é¡ä¼¼åº¦è¨ˆç®—": {
                                "ã‚¹ã‚³ã‚¢": round(doc[1], 4)
                            }
                        }
                    }
                }
                search_details.append(detail)
            
            print(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
            print(f"æ¤œç´¢çµæœæ•°: {len(filtered_docs)}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
            for detail in search_details:
                print(f"ã‚¹ã‚³ã‚¢: {detail['ã‚¹ã‚³ã‚¢']}, ãƒ†ã‚­ã‚¹ãƒˆ: {detail['ãƒ†ã‚­ã‚¹ãƒˆ']}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
            
            return context_text, search_details
            
        except Exception as e:
            error_message = str(e)
            if "insufficient_quota" in error_message:
                print("\nğŸš¨ Critical: API quota has been exceeded!")
                print("Please check your OpenAI API key and billing settings.")
                print("You can check your usage and quota at: https://platform.openai.com/account/usage")
                # ç©ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¿”ã™
                return "", [{
                    "ã‚¨ãƒ©ãƒ¼": True,
                    "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": "API quota has been exceeded",
                    "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "API Quota Error",
                    "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": "Please update your API key in Streamlit Cloud settings"
                }]
            else:
                print(f"\nâŒ Error in get_relevant_context: {error_message}")
                return "", [{
                    "ã‚¨ãƒ©ãƒ¼": True,
                    "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": error_message,
                    "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "Unknown Error"
                }]

    def get_response(self, query: str, system_prompt: str = None, response_template: str = None, property_info: str = None, chat_history: list = None) -> Tuple[str, Dict[str, Any]]:
        """ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­å®š
            system_prompt = system_prompt or self.system_prompt
            response_template = response_template or self.response_template
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®ä½œæˆ
            messages = [
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("system", "å‚ç…§æ–‡è„ˆ:\n{context}")
            ]
            
            # ç‰©ä»¶æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            if property_info:
                messages.append(("system", "ç‰©ä»¶æƒ…å ±:\n{property_info}"))
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®è¿½åŠ 
            messages.append(("human", "{input}"))
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
            prompt = ChatPromptTemplate.from_messages(messages)
            
            # ãƒã‚§ãƒ¼ãƒ³ã®åˆæœŸåŒ–
            chain = prompt | self.llm
            
            # é–¢é€£ã™ã‚‹æ–‡è„ˆã‚’å–å¾—
            context, search_details = self.get_relevant_context(query)
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¨­å®š
            if chat_history:
                self.message_history.messages = []
                for role, content in chat_history:
                    if role == "human":
                        self.message_history.add_user_message(content)
                    elif role == "ai":
                        self.message_history.add_ai_message(content)
            
            # ä¼šè©±å±¥æ­´ã‚’æœ€é©åŒ–
            self.optimize_chat_history()
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            prompt_tokens = self.count_tokens(system_prompt)
            print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {prompt_tokens}")
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            history_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
            print(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {history_tokens}")
            
            # å¿œç­”ã‚’ç”Ÿæˆ
            response = chain.invoke({
                "chat_history": self.message_history.messages,
                "context": context,
                "property_info": property_info or "ç‰©ä»¶æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "input": query
            })
            
            # å¿œç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            response_tokens = self.count_tokens(response.content)
            print(f"å¿œç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {response_tokens}")
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            self.message_history.add_user_message(query)
            self.message_history.add_ai_message(response.content)
            
            # è©³ç´°æƒ…å ±ã®ä½œæˆ
            details = {
                "ãƒ¢ãƒ‡ãƒ«": "gpt-3.5-turbo",
                "ä¼šè©±å±¥æ­´": "æœ‰åŠ¹",
                "ãƒˆãƒ¼ã‚¯ãƒ³æ•°": {
                    "ã‚¯ã‚¨ãƒª": self.count_tokens(query),
                    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": prompt_tokens,
                    "ãƒãƒ£ãƒƒãƒˆå±¥æ­´": history_tokens,
                    "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ": self.count_tokens(context),
                    "å¿œç­”": response_tokens,
                    "åˆè¨ˆ": prompt_tokens + history_tokens + self.count_tokens(context) + response_tokens
                },
                "æ–‡è„ˆæ¤œç´¢": {
                    "æ¤œç´¢çµæœæ•°": len(search_details),
                    "ãƒãƒƒãƒã—ãŸãƒãƒ£ãƒ³ã‚¯": search_details
                },
                "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": {
                    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": system_prompt,
                    "å¿œç­”ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ": response_template
                },
                "ç‰©ä»¶æƒ…å ±": property_info or "ç‰©ä»¶æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "ä¼šè©±å±¥æ­´æ•°": len(chat_history) if chat_history else 0
            }
            
            return response.content, details
            
        except Exception as e:
            error_message = str(e)
            if "insufficient_quota" in error_message:
                error_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚APIã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚\n\n" + \
                               "ä»¥ä¸‹ã®æ‰‹é †ã§å¯¾å¿œã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼š\n" + \
                               "1. OpenAIã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„\n" + \
                               "2. æ–°ã—ã„APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãã ã•ã„\n" + \
                               "3. Streamlit Cloudã®è¨­å®šã§æ–°ã—ã„APIã‚­ãƒ¼ã‚’æ›´æ–°ã—ã¦ãã ã•ã„\n\n" + \
                               "è©³ç´°ã¯ã“ã¡ã‚‰ã§ç¢ºèªã§ãã¾ã™ï¼š\n" + \
                               "https://platform.openai.com/account/usage"
            else:
                error_response = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{error_message}"
            
            error_details = {
                "ã‚¨ãƒ©ãƒ¼": True,
                "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": error_message,
                "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "API Quota Error" if "insufficient_quota" in error_message else "Unknown Error"
            }
            
            return error_response, error_details

    def optimize_chat_history(self, max_tokens: int = 12000) -> None:
        """ä¼šè©±å±¥æ­´ã‚’æœ€é©åŒ–ã—ã€é‡è¦ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä¿æŒ"""
        if not self.message_history.messages:
            return

        # ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        current_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
        if current_tokens <= max_tokens:
            return

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é‡è¦åº¦ã§åˆ†é¡
        important_messages = []
        other_messages = []
        
        for msg in self.message_history.messages:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å¸¸ã«ä¿æŒ
            if isinstance(msg, SystemMessage):
                important_messages.append(msg)
                continue
                
            # æœ€å¾Œã®Nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ä¿æŒï¼ˆNã¯è¨­å®šå¯èƒ½ï¼‰
            if len(important_messages) < 4:  # æœ€å¾Œã®4ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿æŒ
                important_messages.append(msg)
                continue
                
            # ãã®ä»–ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ä¸€æ™‚çš„ã«ä¿å­˜
            other_messages.append(msg)

        # é‡è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        important_tokens = sum(self.count_tokens(msg.content) for msg in important_messages)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        remaining_tokens = max_tokens - important_tokens
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ã„ã¦ã€ä»–ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        for msg in reversed(other_messages):
            msg_tokens = self.count_tokens(msg.content)
            if msg_tokens <= remaining_tokens:
                important_messages.insert(0, msg)  # å…ˆé ­ã«è¿½åŠ 
                remaining_tokens -= msg_tokens
            else:
                break

        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§å±¥æ­´ã‚’æ›´æ–°
        self.message_history.messages = important_messages

    def clear_memory(self):
        """ä¼šè©±ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"""
        self.message_history.clear() 