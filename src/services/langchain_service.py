from typing import List, Dict, Any, Tuple, Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, AIMessage, SystemMessage
import os
import tiktoken
from openai import OpenAI
import streamlit as st
from ..config.settings import (
    PINECONE_API_KEY,
    PINECONE_INDEX_NAME,
    OPENAI_API_KEY,
    DEFAULT_TOP_K,
    SIMILARITY_THRESHOLD,
    DEFAULT_SYSTEM_PROMPT,
    DEFAULT_RESPONSE_TEMPLATE
)

class LangChainService:
    def __init__(self, callback_manager=None):
        """LangChainã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.openai_client = OpenAI(api_key=OPENAI_API_KEY)
        
        # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.llm = ChatOpenAI(
            api_key=OPENAI_API_KEY,
            model_name="gpt-4o-mini",
            temperature=0.85,
            callback_manager=callback_manager
        )
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embeddings = OpenAIEmbeddings(
            api_key=OPENAI_API_KEY,
            model="text-embedding-3-large",
            dimensions=3072
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        
        # Pineconeã®APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
        if PINECONE_API_KEY:
            os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY
        
        # Pineconeãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
        self.vectorstore = PineconeVectorStore.from_existing_index(
            index_name=PINECONE_INDEX_NAME,
            embedding=self.embeddings
        )
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
        self.message_history = ChatMessageHistory()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.system_prompt = DEFAULT_SYSTEM_PROMPT
        self.response_template = DEFAULT_RESPONSE_TEMPLATE

    def check_api_usage(self):
        """OpenAI APIã®ä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª"""
        try:
            # ä½¿ç”¨çŠ¶æ³ã®å–å¾—
            # usage = self.openai_client.usage.retrieve()
            
            # ä½¿ç”¨çŠ¶æ³ã®è¡¨ç¤º
            print("\n=== OpenAI API Usage ===")
            # print(f"Total Tokens: {usage.total_tokens}")
            # print(f"Total Cost: ${usage.total_cost:.4f}")
            # print(f"Usage Period: {usage.period}")
            
            # ã‚¯ã‚©ãƒ¼ã‚¿æƒ…å ±ã®å–å¾—
            # quota = self.openai_client.quota.retrieve()
            print("\n=== OpenAI API Quota ===")
            # print(f"Total Quota: ${quota.total_quota:.2f}")
            # print(f"Used Quota: ${quota.used_quota:.2f}")
            # print(f"Remaining Quota: ${quota.remaining_quota:.2f}")
            # print(f"Quota Period: {quota.period}")
            
            # è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            # if quota.remaining_quota < 1.0:
            #     print("\nâš ï¸ Warning: Remaining quota is less than $1.0")
            # if quota.remaining_quota < 0.1:
            #     print("ğŸš¨ Critical: Remaining quota is less than $0.1")
                
        except Exception as e:
            error_message = str(e)
            print(f"\nâŒ Error checking API usage: {error_message}")
            
            if "insufficient_quota" in error_message:
                print("\nğŸš¨ Critical: API quota has been exceeded!")
                print("Please check your OpenAI API key and billing settings.")
                print("You can check your usage and quota at: https://platform.openai.com/account/usage")
            elif "object has no attribute" in error_message:
                print("\nâš ï¸ Warning: Unable to check API usage. This might be due to API changes or permissions.")
                print("Please check your OpenAI API key and ensure it has the necessary permissions.")
            else:
                print("\nâš ï¸ Warning: Unable to check API usage. Please verify your API key and permissions.")

    def count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return len(self.encoding.encode(text))

    @st.cache_data(ttl=600)
    def get_relevant_context(_self, query: str, top_k: int = DEFAULT_TOP_K) -> Tuple[str, List[Dict[str, Any]], int]:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡è„ˆã‚’å–å¾—"""
        try:
            # ã‚¯ã‚¨ãƒªã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            query_tokens = _self.count_tokens(query)
            print(f"ã‚¯ã‚¨ãƒªã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {query_tokens}")
            
            # ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_vector = _self.embeddings.embed_query(query)
            
            # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—ã—ã¦ã€å¾Œã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            docs = _self.vectorstore.similarity_search_with_score(query, k=top_k * 2)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç°¡ç•¥åŒ–ã—ã¦ä¿æŒ
            simplified_docs = []
            for doc in docs:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç°¡ç•¥åŒ–
                simplified_metadata = {}
                for key, value in doc[0].metadata.items():
                    if isinstance(value, str):
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å€¤ã‚’çŸ­ãã™ã‚‹ï¼ˆæœ€å¤§100æ–‡å­—ï¼‰
                        simplified_metadata[key] = value[:100] + "..." if len(value) > 100 else value
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’çŸ­ãã™ã‚‹ï¼ˆæœ€å¤§500æ–‡å­—ï¼‰
                content = doc[0].page_content
                if len(content) > 500:
                    content = content[:500] + "..."
                
                # ç°¡ç•¥åŒ–ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
                simplified_doc = {
                    "content": content,
                    "metadata": simplified_metadata,
                    "score": doc[1]
                }
                simplified_docs.append(simplified_doc)
            
            # ã‚¹ã‚³ã‚¢ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_docs = []
            for doc in simplified_docs:
                if doc["score"] >= SIMILARITY_THRESHOLD:
                    filtered_docs.append(doc)
                    if len(filtered_docs) >= top_k:
                        break
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®çµæœãŒ0ä»¶ã®å ´åˆã¯ã€ã‚¹ã‚³ã‚¢ã«é–¢ä¿‚ãªãä¸Šä½Kä»¶ã‚’ä½¿ç”¨
            if not filtered_docs and simplified_docs:
                filtered_docs = simplified_docs[:top_k]
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ãªã„ï¼‰
            context_text = "\n".join([doc["content"] for doc in filtered_docs])
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            context_tokens = _self.count_tokens(context_text)
            print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {context_tokens}")
            
            search_details = []
            for doc in filtered_docs:
                detail = {
                    "ã‚¹ã‚³ã‚¢": round(doc["score"], 4),
                    "ãƒ†ã‚­ã‚¹ãƒˆ": doc["content"][:100] + "...",
                    "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿": doc["metadata"],
                    "ãƒ•ã‚¡ã‚¤ãƒ«å": doc["metadata"].get("source", "ä¸æ˜"),
                    "ãƒšãƒ¼ã‚¸ç•ªå·": doc["metadata"].get("page", "ä¸æ˜"),
                    "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": doc["metadata"].get("section", "ä¸æ˜")
                }
                search_details.append(detail)
            
            return context_text, search_details, context_tokens
            
        except Exception as e:
            error_message = str(e)
            if "insufficient_quota" in error_message:
                print("\nğŸš¨ Critical: API quota has been exceeded!")
                print("Please check your OpenAI API key and billing settings.")
                print("You can check your usage and quota at: https://platform.openai.com/account/usage")
                return "", [{
                    "ã‚¨ãƒ©ãƒ¼": True,
                    "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": "API quota has been exceeded",
                    "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "API Quota Error",
                    "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": "Please update your API key in Streamlit Cloud settings"
                }], 0
            else:
                print(f"\nâŒ Error in get_relevant_context: {error_message}")
                return "", [{
                    "ã‚¨ãƒ©ãƒ¼": True,
                    "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": error_message,
                    "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "Unknown Error"
                }], 0

    def get_response(self, query: str, system_prompt: Optional[str] = None, response_template: Optional[str] = None, property_info: Optional[str] = None, chat_history: Optional[list] = None) -> Tuple[str, Dict[str, Any]]:
        """ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­å®š
            system_prompt = system_prompt or self.system_prompt
            response_template = response_template or self.response_template
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®ä½œæˆ
            messages = [
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("system", "å‚ç…§æ–‡è„ˆ:\n{context}")
            ]
            
            # ç‰©ä»¶æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            if property_info:
                messages.append(("system", "ç‰©ä»¶æƒ…å ±:\n{property_info}"))
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®è¿½åŠ 
            messages.append(("human", "{input}"))
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
            prompt = ChatPromptTemplate.from_messages(messages)
            
            # ãƒã‚§ãƒ¼ãƒ³ã®åˆæœŸåŒ–
            chain = prompt | self.llm
            
            # é–¢é€£ã™ã‚‹æ–‡è„ˆã‚’å–å¾—
            context, search_details, context_tokens = self.get_relevant_context(query)
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¨­å®š
            if chat_history:
                self.message_history.messages = []
                for role, content in chat_history:
                    if role == "human":
                        self.message_history.add_user_message(content)
                    elif role == "ai":
                        self.message_history.add_ai_message(content)
            
            # ä¼šè©±å±¥æ­´ã‚’æœ€é©åŒ–
            self.optimize_chat_history()
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            prompt_tokens = self.count_tokens(system_prompt or "")
            print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {prompt_tokens}")
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            history_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
            print(f"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {history_tokens}")
            
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼šé€ä¿¡ã•ã‚Œã‚‹ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
            print("\n=== é€ä¿¡ã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆ ===")
            print("\n--- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---")
            print(system_prompt)
            print("\n--- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ ---")
            for msg in self.message_history.messages:
                print(f"\n[{msg.type}]: {msg.content}")
            print("\n--- å‚ç…§æ–‡è„ˆ ---")
            print(context)
            if property_info:
                print("\n--- ç‰©ä»¶æƒ…å ± ---")
                print(property_info)
            print("\n--- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ---")
            print(query)
            
            # å¿œç­”ã‚’ç”Ÿæˆ
            response = chain.invoke({
                "chat_history": self.message_history.messages,
                "context": context,
                "property_info": property_info or "ç‰©ä»¶æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "input": query
            })
            
            # å¿œç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            response_tokens = self.count_tokens(response.content)
            print(f"å¿œç­”ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {response_tokens}")
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
            self.message_history.add_user_message(query)
            self.message_history.add_ai_message(response.content)
            
            # è©³ç´°æƒ…å ±ã®ä½œæˆ
            details = {
                "ãƒ¢ãƒ‡ãƒ«": "gpt-4o-mini",
                "ä¼šè©±å±¥æ­´": "æœ‰åŠ¹",
                "ãƒˆãƒ¼ã‚¯ãƒ³æ•°": {
                    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": prompt_tokens,
                    "ãƒãƒ£ãƒƒãƒˆå±¥æ­´": history_tokens,
                    "å‚ç…§æ–‡è„ˆ": context_tokens,
                    "ç‰©ä»¶æƒ…å ±": self.count_tokens(property_info) if property_info else 0,
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›": self.count_tokens(query),
                    "åˆè¨ˆ": prompt_tokens + history_tokens + context_tokens + (self.count_tokens(property_info) if property_info else 0)
                },
                "é€ä¿¡ãƒ†ã‚­ã‚¹ãƒˆ": {
                    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ": system_prompt,
                    "ãƒãƒ£ãƒƒãƒˆå±¥æ­´": [{"type": msg.type, "content": msg.content} for msg in self.message_history.messages],
                    "å‚ç…§æ–‡è„ˆ": context,
                    "å‚ç…§æ–‡è„ˆã®è©³ç´°": search_details,
                    "ç‰©ä»¶æƒ…å ±": property_info,
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›": query
                }
            }
            
            return response.content, details
            
        except Exception as e:
            error_message = str(e)
            if "insufficient_quota" in error_message:
                error_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚APIã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚\n\n" + \
                               "ä»¥ä¸‹ã®æ‰‹é †ã§å¯¾å¿œã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼š\n" + \
                               "1. OpenAIã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„\n" + \
                               "2. æ–°ã—ã„APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãã ã•ã„\n" + \
                               "3. Streamlit Cloudã®è¨­å®šã§æ–°ã—ã„APIã‚­ãƒ¼ã‚’æ›´æ–°ã—ã¦ãã ã•ã„\n\n" + \
                               "è©³ç´°ã¯ã“ã¡ã‚‰ã§ç¢ºèªã§ãã¾ã™ï¼š\n" + \
                               "https://platform.openai.com/account/usage"
            else:
                error_response = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{error_message}"
            
            error_details = {
                "ã‚¨ãƒ©ãƒ¼": True,
                "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸": error_message,
                "ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—": "API Quota Error" if "insufficient_quota" in error_message else "Unknown Error"
            }
            
            return error_response, error_details

    def optimize_chat_history(self, max_tokens: int = 10000) -> None:
        """ä¼šè©±å±¥æ­´ã‚’æœ€é©åŒ–ã—ã€é‡è¦ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä¿æŒ"""
        if not self.message_history.messages:
            return

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ç¢ºä¿ï¼ˆç´„4000ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        reserved_tokens = 4000
        available_tokens = max_tokens - reserved_tokens

        # ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        current_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
        if current_tokens <= available_tokens:
            return

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é‡è¦åº¦ã§åˆ†é¡
        important_messages = []
        other_messages = []
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿æŒ
        for msg in self.message_history.messages:
            if isinstance(msg, SystemMessage):
                important_messages.append(msg)
                continue
            other_messages.append(msg)

        # æœ€æ–°ã®1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’ä¿æŒ
        if other_messages:
            important_messages.append(other_messages[-1])
            other_messages = other_messages[:-1]

        # é‡è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        important_tokens = sum(self.count_tokens(msg.content) for msg in important_messages)
        
        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        remaining_tokens = available_tokens - important_tokens

        # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ã„ã¦ã€ä»–ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é•·ã•ã§ã‚½ãƒ¼ãƒˆï¼ˆçŸ­ã„ã‚‚ã®ã‹ã‚‰ï¼‰
        other_messages.sort(key=lambda x: self.count_tokens(x.content))
        
        for msg in other_messages:
            msg_tokens = self.count_tokens(msg.content)
            if msg_tokens <= remaining_tokens:
                important_messages.insert(0, msg)  # å…ˆé ­ã«è¿½åŠ 
                remaining_tokens -= msg_tokens
            else:
                break

        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§å±¥æ­´ã‚’æ›´æ–°
        self.message_history.messages = important_messages

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›
        final_tokens = sum(self.count_tokens(msg.content) for msg in self.message_history.messages)
        print(f"\n=== Chat History Optimization ===")
        print(f"Original tokens: {current_tokens}")
        print(f"Final tokens: {final_tokens}")
        print(f"Messages kept: {len(self.message_history.messages)}")
        print(f"Available tokens: {available_tokens}")
        print(f"Remaining tokens: {remaining_tokens}")

    def clear_memory(self):
        """ä¼šè©±ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"""
        self.message_history.clear()    