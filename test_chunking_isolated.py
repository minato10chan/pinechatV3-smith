#!/usr/bin/env python3
"""
Completely isolated test for chunking logic without any Streamlit dependencies
"""
import json
import sys
import tiktoken

def split_property_data_isolated(property_data: dict, max_tokens: int = 8000) -> list:
    """Isolated version of property data splitting function"""
    encoding = tiktoken.encoding_for_model("text-embedding-3-large")
    
    base_info = {
        "property_name": property_data["property_name"],
        "property_type": property_data["property_type"],
        "prefecture": property_data["prefecture"],
        "city": property_data["city"],
        "detailed_address": property_data["detailed_address"],
        "latitude": property_data.get("latitude", "0.0"),
        "longitude": property_data.get("longitude", "0.0")
    }
    
    details = property_data.get("property_details", "")
    if not details:
        return [{"text": json.dumps(base_info, ensure_ascii=False), "metadata": base_info}]
    
    paragraphs = [p.strip() for p in details.split('\n') if p.strip()]
    
    split_paragraphs = []
    for paragraph in paragraphs:
        paragraph_tokens = len(encoding.encode(paragraph))
        
        if paragraph_tokens <= max_tokens:
            split_paragraphs.append(paragraph)
        else:
            sentences = [s.strip() for s in paragraph.replace('ã€‚', 'ã€‚\n').split('\n') if s.strip()]
            current_sentence_group = []
            current_group_tokens = 0
            
            for sentence in sentences:
                sentence_tokens = len(encoding.encode(sentence))
                
                if current_group_tokens + sentence_tokens > max_tokens:
                    if current_sentence_group:
                        split_paragraphs.append(''.join(current_sentence_group))
                    current_sentence_group = [sentence]
                    current_group_tokens = sentence_tokens
                else:
                    current_sentence_group.append(sentence)
                    current_group_tokens += sentence_tokens
            
            if current_sentence_group:
                split_paragraphs.append(''.join(current_sentence_group))
    
    chunks = []
    current_chunk = []
    
    for paragraph in split_paragraphs:
        if current_chunk:
            test_text = "\n".join(current_chunk + [paragraph])
        else:
            test_text = paragraph
        
        test_tokens = len(encoding.encode(test_text))
        
        if test_tokens > max_tokens:
            if current_chunk:
                chunk_info = base_info.copy()
                chunk_info["property_details"] = "\n".join(current_chunk)
                chunk_info["chunk_number"] = len(chunks) + 1
                
                chunk_text = json.dumps(chunk_info, ensure_ascii=False)
                chunk = {
                    "text": chunk_text,
                    "metadata": chunk_info
                }
                chunks.append(chunk)
            
            current_chunk = [paragraph]
        else:
            current_chunk.append(paragraph)
    
    if current_chunk:
        chunk_info = base_info.copy()
        chunk_info["property_details"] = "\n".join(current_chunk)
        chunk_info["chunk_number"] = len(chunks) + 1
        
        chunk_text = json.dumps(chunk_info, ensure_ascii=False)
        chunk = {
            "text": chunk_text,
            "metadata": chunk_info
        }
        chunks.append(chunk)
    
    for chunk in chunks:
        chunk["metadata"]["total_chunks"] = len(chunks)
    
    return chunks

def test_isolated_chunking():
    """Test chunking functionality in complete isolation"""
    print("=== Isolated Property Text Chunking Test ===")
    
    try:
        long_description = """
ã“ã®ç‰©ä»¶ã¯æ±äº¬éƒ½å¿ƒéƒ¨ã«ä½ç½®ã™ã‚‹é«˜ç´šãƒãƒ³ã‚·ãƒ§ãƒ³ã§ã™ã€‚æœ€å¯„ã‚Šé§…ã‹ã‚‰å¾’æ­©3åˆ†ã¨ã„ã†æŠœç¾¤ã®ç«‹åœ°æ¡ä»¶ã‚’èª‡ã‚Šã€éƒ½å¿ƒã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒéå¸¸ã«ä¾¿åˆ©ã§ã™ã€‚å»ºç‰©ã¯åœ°ä¸Š20éšå»ºã¦ã®é‰„ç­‹ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆé€ ã§ã€2022å¹´ã«ç«£å·¥ã—ãŸæ–°ç¯‰ç‰©ä»¶ã§ã™ã€‚

å¤–è¦³ã¯æ´—ç·´ã•ã‚ŒãŸãƒ¢ãƒ€ãƒ³ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’æ¡ç”¨ã—ã€ã‚¨ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ã«ã¯24æ™‚é–“æœ‰äººç®¡ç†ã®ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥ã‚µãƒ¼ãƒ“ã‚¹ãŒé…ç½®ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é¢ã§ã¯ã€ã‚ªãƒ¼ãƒˆãƒ­ãƒƒã‚¯ã€é˜²çŠ¯ã‚«ãƒ¡ãƒ©ã€å®…é…ãƒœãƒƒã‚¯ã‚¹ãªã©æœ€æ–°ã®è¨­å‚™ãŒå®Œå‚™ã•ã‚Œã¦ã„ã¾ã™ã€‚

å„ä½æˆ¸ã¯å—å‘ãã®è§’éƒ¨å±‹ã‚’ä¸­å¿ƒã¨ã—ãŸè¨­è¨ˆã§ã€å¤§ããªçª“ã‹ã‚‰è±Šå¯Œãªè‡ªç„¶å…‰ãŒå·®ã—è¾¼ã¿ã¾ã™ã€‚å®¤å†…ã¯é«˜ç´šæ„Ÿã®ã‚ã‚‹å†…è£…ä»•ä¸Šã’ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒƒãƒãƒ³ã€æµ´å®¤ä¹¾ç‡¥æ©Ÿã€åºŠæš–æˆ¿ã€ã‚¨ã‚¢ã‚³ãƒ³ãªã©ã®è¨­å‚™ãŒæ¨™æº–è£…å‚™ã•ã‚Œã¦ã„ã¾ã™ã€‚

å…±ç”¨æ–½è¨­ã¨ã—ã¦ã€å±‹ä¸Šåº­åœ’ã€ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚¸ãƒ ã€ã‚²ã‚¹ãƒˆãƒ«ãƒ¼ãƒ ã€ã‚­ãƒƒã‚ºãƒ«ãƒ¼ãƒ ã€ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ãƒ«ãƒ¼ãƒ ãªã©ãŒç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€ä½æ°‘ã®å¿«é©ãªç”Ÿæ´»ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ã¾ãŸã€æ•·åœ°å†…ã«ã¯æ¥å®¢ç”¨é§è»Šå ´ã‚‚å®Œå‚™ã•ã‚Œã¦ã„ã¾ã™ã€‚

å‘¨è¾ºç’°å¢ƒã¯éå¸¸ã«å……å®Ÿã—ã¦ãŠã‚Šã€å¾’æ­©åœå†…ã«ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ¼ã‚±ãƒƒãƒˆã€ã‚³ãƒ³ãƒ“ãƒ‹ã‚¨ãƒ³ã‚¹ã‚¹ãƒˆã‚¢ã€éŠ€è¡Œã€éƒµä¾¿å±€ã€ç—…é™¢ã€è–¬å±€ãªã©ã®ç”Ÿæ´»ã«å¿…è¦ãªæ–½è¨­ãŒæƒã£ã¦ã„ã¾ã™ã€‚ã¾ãŸã€è¿‘éš£ã«ã¯æœ‰åãªå…¬åœ’ãŒã‚ã‚Šã€ç·‘è±Šã‹ãªç’°å¢ƒã§ãƒªãƒ©ãƒƒã‚¯ã‚¹ã§ãã¾ã™ã€‚

æ•™è‚²ç’°å¢ƒã‚‚å„ªã‚Œã¦ãŠã‚Šã€è©•åˆ¤ã®è‰¯ã„å°å­¦æ ¡ã€ä¸­å­¦æ ¡ãŒå­¦åŒºå†…ã«ã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€æœ‰åç§ç«‹å­¦æ ¡ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚‚è‰¯å¥½ã§ã€å­è‚²ã¦ä¸–ä»£ã«ã¨ã£ã¦ç†æƒ³çš„ãªç’°å¢ƒã§ã™ã€‚ä¿è‚²åœ’ã‚„å¹¼ç¨šåœ’ã‚‚è¤‡æ•°ã‚ã‚Šã€å¾…æ©Ÿå…ç«¥ã®å¿ƒé…ã‚‚å°‘ãªã„åœ°åŸŸã§ã™ã€‚

äº¤é€šã‚¢ã‚¯ã‚»ã‚¹ã¯è¤‡æ•°è·¯ç·šãŒåˆ©ç”¨å¯èƒ½ã§ã€ä¸»è¦ã‚¿ãƒ¼ãƒŸãƒŠãƒ«é§…ã¾ã§ä¹—ã‚Šæ›ãˆãªã—ã§ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚æœã®é€šå‹¤ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã§ã‚‚æ¯”è¼ƒçš„æ··é›‘ãŒå°‘ãªãã€å¿«é©ãªé€šå‹¤ãŒå¯èƒ½ã§ã™ã€‚ã¾ãŸã€ç¾½ç”°ç©ºæ¸¯ã‚„æˆç”°ç©ºæ¸¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚‚è‰¯å¥½ã§ã€å‡ºå¼µã‚„æ—…è¡Œã®éš›ã«ã‚‚ä¾¿åˆ©ã§ã™ã€‚

å•†æ¥­æ–½è¨­ã‚‚å……å®Ÿã—ã¦ãŠã‚Šã€å¤§å‹ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ«ã€ãƒ‡ãƒ‘ãƒ¼ãƒˆã€å°‚é–€åº—è¡—ãªã©ãŒè¿‘éš£ã«ã‚ã‚Šã¾ã™ã€‚ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã€ã‚«ãƒ•ã‚§ã€å±…é…’å±‹ãªã©ã‚‚å¤šæ•°ã‚ã‚Šã€å¤–é£Ÿã‚„ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆã«ã‚‚å›°ã‚Šã¾ã›ã‚“ã€‚

å°†æ¥æ€§ã«ã¤ã„ã¦ã‚‚ã€ã“ã®åœ°åŸŸã¯å†é–‹ç™ºè¨ˆç”»ãŒé€²è¡Œä¸­ã§ã€ã•ã‚‰ãªã‚‹ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚æ–°ã—ã„å•†æ¥­æ–½è¨­ã‚„å…¬å…±æ–½è¨­ã®å»ºè¨­ã‚‚äºˆå®šã•ã‚Œã¦ãŠã‚Šã€è³‡ç”£ä¾¡å€¤ã®å‘ä¸Šã‚‚è¦‹è¾¼ã¾ã‚Œã¾ã™ã€‚

ç®¡ç†ä½“åˆ¶ã¯ä¿¡é ¼ã§ãã‚‹å¤§æ‰‹ç®¡ç†ä¼šç¤¾ãŒæ‹…å½“ã—ã€å»ºç‰©ã®ç¶­æŒç®¡ç†ã€æ¸…æƒã€è¨­å‚™ç‚¹æ¤œãªã©ãŒé©åˆ‡ã«è¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ç®¡ç†è²»ã‚„ä¿®ç¹•ç©ç«‹é‡‘ã‚‚é©æ­£ãªæ°´æº–ã«è¨­å®šã•ã‚Œã¦ãŠã‚Šã€é•·æœŸçš„ãªè³‡ç”£ä¾¡å€¤ã®ç¶­æŒãŒæœŸå¾…ã§ãã¾ã™ã€‚

ã“ã®ç‰©ä»¶ã®æœ€å¤§ã®é­…åŠ›ã¯ã€éƒ½å¿ƒéƒ¨ã§ã‚ã‚ŠãªãŒã‚‰é™ã‹ã§è½ã¡ç€ã„ãŸä½ç’°å¢ƒã‚’æä¾›ã—ã¦ã„ã‚‹ã“ã¨ã§ã™ã€‚è¿‘éš£ã«ã¯ç·‘è±Šã‹ãªå…¬åœ’ã‚„æ•£æ­©é“ãŒã‚ã‚Šã€éƒ½å¸‚ç”Ÿæ´»ã®ä¸­ã§ã‚‚è‡ªç„¶ã‚’æ„Ÿã˜ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€å»ºç‰©ã®è¨­è¨ˆã«ã‚‚ç’°å¢ƒã¸ã®é…æ…®ãŒè¦‹ã‚‰ã‚Œã€çœã‚¨ãƒãƒ«ã‚®ãƒ¼è¨­å‚™ã‚„å¤ªé™½å…‰ç™ºé›»ã‚·ã‚¹ãƒ†ãƒ ãªã©ãŒå°å…¥ã•ã‚Œã¦ã„ã¾ã™ã€‚

ä½æˆ¸ã®é–“å–ã‚Šã¯å¤šæ§˜ã§ã€å˜èº«è€…å‘ã‘ã®1Kã‹ã‚‰å®¶æ—å‘ã‘ã®3LDKã¾ã§å¹…åºƒãç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚å„ä½æˆ¸ã«ã¯æœ€æ–°ã®è¨­å‚™ãŒå®Œå‚™ã•ã‚Œã¦ãŠã‚Šã€å¿«é©ãªç”Ÿæ´»ã‚’é€ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç‰¹ã«ã€ã‚­ãƒƒãƒãƒ³ã¯é«˜ç´šãƒ–ãƒ©ãƒ³ãƒ‰ã®è¨­å‚™ã‚’ä½¿ç”¨ã—ã€æ–™ç†å¥½ãã®æ–¹ã«ã‚‚æº€è¶³ã—ã¦ã„ãŸã ã‘ã‚‹ã§ã—ã‚‡ã†ã€‚

ãƒã‚¹ãƒ«ãƒ¼ãƒ ã‚‚åºƒã€…ã¨ã—ãŸè¨­è¨ˆã§ã€ä¸€æ—¥ã®ç–²ã‚Œã‚’ç™’ã™ã“ã¨ãŒã§ãã¾ã™ã€‚æµ´å®¤ä¹¾ç‡¥æ©Ÿã‚„è¿½ã„ç„šãæ©Ÿèƒ½ãªã©ã‚‚æ¨™æº–è£…å‚™ã•ã‚Œã¦ãŠã‚Šã€å¿«é©ãªãƒã‚¹ã‚¿ã‚¤ãƒ ã‚’æ¥½ã—ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

åç´ã‚¹ãƒšãƒ¼ã‚¹ã‚‚è±Šå¯Œã«ç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€å­£ç¯€ã®è¡£é¡ã‚„æ—¥ç”¨å“ãªã©ã‚’ã™ã£ãã‚Šã¨æ•´ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¦ã‚©ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ã‚¼ãƒƒãƒˆã‚„åºŠä¸‹åç´ãªã©ã€æ§˜ã€…ãªã‚¿ã‚¤ãƒ—ã®åç´ãŒé…ç½®ã•ã‚Œã¦ã„ã¾ã™ã€‚

å…±ç”¨éƒ¨åˆ†ã®ç®¡ç†ã‚‚è¡Œãå±Šã„ã¦ãŠã‚Šã€ã‚¨ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ›ãƒ¼ãƒ«ã¯å¸¸ã«æ¸…æ½”ã«ä¿ãŸã‚Œã¦ã„ã¾ã™ã€‚å®…é…ãƒœãƒƒã‚¯ã‚¹ã‚‚å¤§å‹ã®ã‚‚ã®ãŒè¨­ç½®ã•ã‚Œã¦ãŠã‚Šã€ä¸åœ¨æ™‚ã§ã‚‚å®‰å¿ƒã—ã¦è·ç‰©ã‚’å—ã‘å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

é§è»Šå ´ã¯æ©Ÿæ¢°å¼ã¨å¹³é¢å¼ã®ä¸¡æ–¹ãŒç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€è»Šã‚’ãŠæŒã¡ã®æ–¹ã«ã‚‚ä¾¿åˆ©ã§ã™ã€‚ã¾ãŸã€è‡ªè»¢è»Šç½®ãå ´ã‚„ãƒã‚¤ã‚¯ç½®ãå ´ã‚‚å®Œå‚™ã•ã‚Œã¦ãŠã‚Šã€æ§˜ã€…ãªäº¤é€šæ‰‹æ®µã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

ãƒšãƒƒãƒˆé£¼è‚²ã«ã¤ã„ã¦ã‚‚ç›¸è«‡å¯èƒ½ã§ã€æ„›çŠ¬ã‚„æ„›çŒ«ã¨ä¸€ç·’ã«æš®ã‚‰ã™ã“ã¨ãŒã§ãã¾ã™ã€‚è¿‘éš£ã«ã¯ãƒšãƒƒãƒˆç—…é™¢ã‚„ãƒšãƒƒãƒˆã‚·ãƒ§ãƒƒãƒ—ã‚‚ã‚ã‚Šã€ãƒšãƒƒãƒˆã¨ã®ç”Ÿæ´»ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ç’°å¢ƒãŒæ•´ã£ã¦ã„ã¾ã™ã€‚

ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆç’°å¢ƒã‚‚å……å®Ÿã—ã¦ãŠã‚Šã€å…‰ãƒ•ã‚¡ã‚¤ãƒãƒ¼ãŒå„ä½æˆ¸ã¾ã§å¼•ãè¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚åœ¨å®…ãƒ¯ãƒ¼ã‚¯ã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ã‚‚å¯¾å¿œã§ãã‚‹é«˜é€Ÿã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆç’°å¢ƒãŒæ•´å‚™ã•ã‚Œã¦ã„ã¾ã™ã€‚

é˜²ç½é¢ã§ã‚‚å®‰å¿ƒã§ã€å»ºç‰©ã¯æœ€æ–°ã®è€éœ‡åŸºæº–ã«é©åˆã—ã¦ãŠã‚Šã€éå¸¸ç”¨ç™ºé›»æ©Ÿã‚„é˜²ç½å‚™è“„å€‰åº«ãªã©ã‚‚å®Œå‚™ã•ã‚Œã¦ã„ã¾ã™ã€‚å®šæœŸçš„ãªé˜²ç½è¨“ç·´ã‚‚å®Ÿæ–½ã•ã‚Œã¦ãŠã‚Šã€ä½æ°‘ã®å®‰å…¨æ„è­˜ã‚‚é«˜ãä¿ãŸã‚Œã¦ã„ã¾ã™ã€‚
""" * 20  # Make it 20x longer to test chunking thoroughly
        
        test_property = {
            'property_name': 'ãƒ†ã‚¹ãƒˆçµ±åˆç‰©ä»¶ï¼ˆå®Œå…¨åˆ†é›¢ç‰ˆï¼‰',
            'property_type': 'ãƒãƒ³ã‚·ãƒ§ãƒ³',
            'prefecture': 'æ±äº¬éƒ½',
            'city': 'æ¸‹è°·åŒº',
            'detailed_address': 'æ¸‹è°·1-1-1',
            'property_details': long_description,
            'latitude': '35.6580',
            'longitude': '139.7016'
        }
        
        print(f"Property details length: {len(long_description):,} characters")
        
        for token_limit in [2000, 8000]:
            print(f"\n--- Testing with {token_limit} token limit ---")
            chunks = split_property_data_isolated(test_property, max_tokens=token_limit)
            print(f"âœ… Created {len(chunks)} chunks with {token_limit} token limit")
            
            all_valid = True
            total_size = 0
            max_chunk_size = 0
            
            for i, chunk in enumerate(chunks):
                chunk_json = chunk["text"]
                chunk_bytes = chunk_json.encode('utf-8')
                chunk_size = len(chunk_bytes)
                total_size += chunk_size
                max_chunk_size = max(max_chunk_size, chunk_size)
                
                print(f"  Chunk {i+1}: {chunk_size:,} bytes")
                
                if chunk_size > 40 * 1024:
                    print(f"  âŒ Chunk {i+1} exceeds 40KB limit!")
                    all_valid = False
                else:
                    print(f"  âœ… Chunk {i+1} is within 40KB limit")
                
                try:
                    metadata = chunk["metadata"]
                    assert "property_name" in metadata
                    assert "chunk_number" in metadata
                    assert "total_chunks" in metadata
                    print(f"  âœ… Chunk {i+1} has valid metadata structure")
                except Exception as e:
                    print(f"  âŒ Chunk {i+1} has invalid metadata: {e}")
                    all_valid = False
            
            print(f"Total size of all chunks: {total_size:,} bytes")
            print(f"Largest chunk size: {max_chunk_size:,} bytes")
            print(f"40KB limit: {40 * 1024:,} bytes")
            
            if all_valid:
                print(f"ğŸ‰ All chunks are valid with {token_limit} token limit!")
            else:
                print(f"âŒ Some chunks are invalid with {token_limit} token limit!")
        
        return True
            
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_isolated_chunking()
    sys.exit(0 if success else 1)
